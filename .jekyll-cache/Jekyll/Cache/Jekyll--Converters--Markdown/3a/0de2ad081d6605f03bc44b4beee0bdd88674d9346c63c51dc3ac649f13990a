I"-<p>Lately I’ve been thinking a lot about alternative ways to make inferences from data. Specifically, I’ve been interested in figuring out better ways to understand relationships between variables using procedures that don’t rely on unreasonable assumptions.</p>

<p>Right now, I’m in the process of working through the <a href="http://course.fast.ai/">FastAI Deep Learning lectures</a>, taught by <a href="https://en.wikipedia.org/wiki/Jeremy_Howard_(entrepreneur)">Jeremy Howard</a> and <a href="https://medium.com/@racheltho">Rachel Thomas</a>. In a <a href="http://forums.fast.ai/t/feeling-unfulfilled-pushing-98-accuracy-on-state-farm/225/8">thread</a> on the course forums, Jeremy mentioned a paper titled <a href="http://projecteuclid.org/euclid.ss/1009213726">Statistical Modeling: The Two Cultures</a>, by Leo Breiman – I was immediately intrigued, as 
it dug into a number of things that I’d already been thinking about. In short, the paper classifies the statistical modeling community into the “data models” faction and the “algorithms” faction, with the former dominating the vast majority of statisticians. As a result, many stasticians focus heavily on data models, often to the exclusion of problems that are better-solved using an algorithmic learning approach. Further, this bias towards data models causes many statisticians to miss out on the fascinating problems that are better-suited to algorithmic modeling.</p>

<p>The typical data modeling process, according to Brieman, often involves picking an analysis technique and then testing the data to ensure that it fits the underlying assumptions (for example, a distribution goodness-of-fit test). If it does, the analyst continues the analysis, confident that the approach is valid. The problem with this approach, however, is that goodness-of-fit tests do not reject a distributional fit unless there is a <em>severe</em> divergence from that distribution. This means that often the analyst is operating as though assumptions hold, when in reality this is false confidence.</p>

<p>From there, the analyst computes some sample parameter of interest and runs a hypothesis test. If the p-value falls below the pretermined significance threshold an effect is assumed to be “real”, and can be freely reported. The weakness of such an approach, however, is that <em>very little consideration is ever given to whether the given model has any chance of accurately describing the underlying relationship between the independent and dependent variables</em>. Brieman asserts that the weakness of the underlying model calls into question the value of the inferences made from it.</p>

<p>Brieman argues that it would be better to focus on finding better predictive models (by, for example, using algorithmic machine learning techniques) than on forcing a problem into a data model that it doesn’t naturally fit. Rather than fitting a linear regression to a series of variables that almost certainly don’t have a linear relationship, focus on using more flexible methods that more accurately model the mapping function between input and output. This approach suggests that, first and foremost, we need to get an accurate model. We can worry about learning from it later.</p>

<p>But this begs the (valid) question: if we focus on finding an accurate predictive algorithm, and this model ends up being a black box, how do we take the model and learn anything from it later? I think this is the right question, and I think there is exciting work to be done in this area. The author goes on at length about using feature importance methods for understanding what variables are driving the model. He briefly mentions performing the same feature importance analysis on a logistic regression, but his focus is really on Random Forests (he actually invented Random Forest models!). The Random Forest variable importance analysis technique is pretty straightforward:</p>
<ol>
  <li>Train and tune the model</li>
  <li>Each of the individual trees in the forest is a bootstrapped sample where a handful of examples are left out (the Out Of Bag – or OOB – samples). Step 2 is to make predictions on the OOB examples</li>
  <li>Then, for each variable, we perturb the values just a bit. We make predictions again for the perturbed-value model, and see how much worse our model gets</li>
  <li>For a given variable, if the predictive accuracy gets substantially worse with the perturbation, we know that variable is important to the overall model, and if the predictive accuracy doesn’t change much, the variable is likely not especially important. The sensitivity of the predictive accuracy to the perturbations is an implicit measure of variable importance</li>
  <li>Do this over every tree in the forest, and compute the average loss in accuracy for a given variable across all trees. The average is the variables importance.</li>
</ol>

<p>This technique fits the Random Forest especially well, given that the very nature of a Random Forest is to build a series of bootstrapped decision trees. But really, <em>any</em> predictive modeling technique can be analyzed with this technique. For example, you could tune a neural network on a full dataset, and then create a handful of bootstrapped datasets from the original data, and follow the algorithm above. It might take some time for more complex and time-consuming models, but no reason we couldn’t use this strategy to understand what inputs are driving any model. Of course, this gets a bit messier when your training data are not in a structured, tabular format, but there may be extensions to other machine learning subdomains that I’m not aware of (for example, I could see a world where we perturb different sections of an image in a computer vision model to determine the importance of that section to the overall power of the model).</p>

<p>I did some digging to see what sort of research is being done in this area. I came across a paper titled <a href="https://arxiv.org/abs/1701.04944">A Machine Learning Alternative to P-values</a> by Min Lu and Hemant Ishwaran that digs into the OOB technique above and expands upon it. They develop a metric called the Variable Importance Index (VIMP) that defines the output of the algorithm described above. I’m very excited to try to implement the VIMP technique in my own projects.</p>

<p>As someone works in an ecommerce setting where useful insights have the potential to have a huge impact, I find these approaches incredibly exciting. I’m excited to experiment with them in my own work. If you’re familiar with any related research, I’d love to hear from you!</p>

<p>Until next time.</p>
:ET