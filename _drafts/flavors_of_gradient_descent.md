---
title: Flavors of Gradient Descent
---

# Basic Stochastic Gradient Descent (SGD)
Basic stochastic gradient descent looks for the partial derivative of the loss function with respect to a given parameter, and subtracts a fraction of that gradient from the parameter itself. The fraction subtracted is called the _learning rate_. In basic SGD, this learning rate is a static number.

# Momentum-Based SGD
Momentum-based SGD instead

# Adagrad

# RMSprop

# Adam

# Eve