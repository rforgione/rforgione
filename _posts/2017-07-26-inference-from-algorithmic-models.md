---
title: Inference from Algorithmic Models
---

Lately I've been thinking a lot about alternative ways to make inferences from data. Specifically, I've been interested in figuring out better ways to understand relationships between variables that don't rely on unreasonable assumptions. 

Right now, I'm in the process of working through the [FastAI Deep Learning lectures](http://course.fast.ai/), taught by Jeremy Howard and Rachel Thomas. In a thread on the course forums, Jeremy mentioned [this](http://projecteuclid.org/euclid.ss/1009213726) paper -- I was immediately intrigued, as 
it summarized a number of things that I've been thinking about around making inferences from black-box models. In short, the paper describes the tendency for stasticians to focus heavily on data models, often to the exclusion of problems that are better-solved using an algorithmic approach. This leads to the overuse of data modeling, and the reluctance of statisticians to dig into algorithmic solutions (and the fascinating problems that come with them).

The process, according to Brieman, often involves picking an analysis technique and then testing the data to ensure that it fits the underlying assumptions (for example, a distribution goodness-of-fit test). If it does, the analyst continues the analysis, confident that the approach is valid. The problem with this approach, however, is that goodness-of-fit tests do not reject conformity to a distribution unless there is a *severe* divergence from the required distribution. This means that often the analyst is operating as though assumptions hold, when in reality this is a false sense of confidence. 

From there, the analyst computes some sample parameter of interest and runs a hypothesis test. If the p-value falls below the pretermined significance threshold an effect is assumed to be "real", and can be freely reported. The weakness of such an approach, however, is that *very little consideration is ever given to whether the given model has any chance of accurately describing the underlying relationship between the independent and dependent variables*. Brieman asserts that the weakness of the underlying model calls into question the value of the inferences made from it.

Brieman argues that it would be better to focus on finding better predictive models (by, for example, using algorithmic machine learning techniques) than on forcing a problem into a data model that it doesn't naturally fit. Rather than fitting a linear regression to a series of variables that almost certainly don't have a linear relationship, focus on using more flexible methods that more accurately model the mapping function between input and output. This approach suggests that, first and foremost, we need to get an accurate model. We can worry about learning from it later.

But this begs the (valid) question: if we focus on finding an accurate predictive algorithm, and this model ends up being a black box, how do we take the model and learn anything from it later? I think this is the right question, and I think there is exciting work to be done in this area. The author seems to *love* Random Forests, and goes on at length about using feature importance methods to understand what variables are driving the model. He briefly mentions performing the same feature importance analysis on a logistic regression, but his focus is really on Random Forests. The Random Forest variable importance analysis technique is pretty straightforward:
1. Train and tune the model
2. Each of the individual trees in the forest is a bootstrapped sample where a handful of examples are left out (the Out Of Bag -- or OOB -- samples). Step 2 is to make predictions on the OOB examples
3. Then, for each variable, we perturb the values just a bit. We make predictions again for the perturbed-value model, and see how much worse our model gets
4. For a given variable, if the predictive accuracy gets substantially worse with the perturbation, we know that variable is important to the overall model, and if the predictive accuracy doesn't change much, the variable is likely not especially important. The sensitivity of the predictive accuracy to the perturbations is an implicit measure of variable importance
5. Do this over every tree in the forest, and compute the average loss in accuracy for a given variable across all trees. The average is the variables importance. 

This technique fits the Random Forest especially well, given that the very nature of a Random Forest is to build a series of bootstrapped decision trees. But really, *any* predictive modeling technique can be analyzed with this technique. For example, you could tune a neural network on a full dataset, and then create a handful of bootstrapped datasets from the original data, and follow the algorithm above. It might take some time for more complex and time-consuming models, but no reason we couldn't use this strategy to understand what inputs are driving any model. Of course, this gets a bit messier when your training data are not in a structured, tabular format, but there may be extensions to other machine learning subdomains that I'm not aware of (for example, I could see a world where we perturb different sections of an image in a computer vision model to determine the importance of that section to the overall power of the model). 

I did some digging to see what sort of research is being done in this area. I came across a paper titled [A Machine Learning Alternative to P-values](https://arxiv.org/abs/1701.04944) by Min Lu and Hemant Ishwaran that digs into the OOB technique above and expands upon it. They develop a metric called the Variable Importance Index (VIMP) that defines the output of the algorithm described above. I'm very excited to try to implement the VIMP technique in my own projects. 

As someone works in an ecommerce setting where useful insights have the potential to have a huge impact, I find these approaches incredibly exciting. I'm excited to experiment with them in my own work. If you're familiar with any related research, I'd love to hear from you!

Until next time. 